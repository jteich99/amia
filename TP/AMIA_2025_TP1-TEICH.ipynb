{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoTVCUtTQL6c"
      },
      "source": [
        "# TP 1: LDA/QDA y optimización matemática de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kL_4etdeizy"
      },
      "source": [
        "# Intro teórica\n",
        "\n",
        "## Definición: Clasificador Bayesiano\n",
        "\n",
        "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
        "\n",
        "De esta manera dicha probabilidad *a posteriori* resulta\n",
        "$$\n",
        "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
        "$$\n",
        "\n",
        "La regla de decisión de Bayes es entonces\n",
        "$$\n",
        "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
        "$$\n",
        "\n",
        "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
        "\n",
        "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
        "\n",
        "## Distribución condicional\n",
        "\n",
        "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
        "\n",
        "Por definición, se tiene entonces que para una clase $j$:\n",
        "$$\n",
        "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
        "$$\n",
        "\n",
        "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
        "\n",
        "## LDA\n",
        "\n",
        "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "## Entrenamiento/Ajuste\n",
        "\n",
        "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
        "\n",
        "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
        "\n",
        "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
        "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
        "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
        "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
        "\n",
        "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
        "\n",
        "## Predicción\n",
        "\n",
        "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV8OF-SlPHbD"
      },
      "source": [
        "# Código provisto\n",
        "\n",
        "Con el fin de no retrasar al alumno con cuestiones estructurales y/o secundarias al tema que se pretende tratar, se provee una base de código que **no es obligatoria de usar** pero se asume que resulta resulta beneficiosa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PrDdJRypNB-y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.linalg as LA\n",
        "from scipy.linalg import cholesky, solve_triangular\n",
        "from scipy.linalg.lapack import dtrtri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cPL33WIN2HA"
      },
      "source": [
        "## Base code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ewg5e0hsNTQC"
      },
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def _estimate_a_priori(self, y):\n",
        "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "    # Q3: para que sirve bincount?\n",
        "    # Rta: cuenta la cantidad de ocurrencias de cada valor de int en un array de ints >= 0. Devuelve un array con cantidad de ocurrencias, donde la posición es el int\n",
        "    # Hay que flatten para llevarlo a un array 1D y desps hacer int para que bincount pueda usarlo\n",
        "    # Lo que termina calculando es una probabilidad a priori considerando una distribución uniforme\n",
        "    # Si no se tiene información previa de \"a priori\", entonces se defaultea a esto que es una suposición no informada razonable\n",
        "    return np.log(a_priori)\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate all needed parameters for given model\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def fit(self, X, y, a_priori=None):\n",
        "    # if it's needed, estimate a priori probabilities\n",
        "    # JIT: si no se tiene -> defaultear a distribución uniforme con _estimate_a_priori\n",
        "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "\n",
        "    # now that everything else is in place, estimate all needed parameters for given model\n",
        "    self._fit_params(X, y)\n",
        "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
        "    # Rta: el a priori se regulariza en logs luego de calcular la probabilidad a priori de cada valor de y\n",
        "    # Luego la predicción va a utilizar esta transformación/regularización de los datos\n",
        "    # Además sin una probabilidad a priori no podemos calcular a posteriori\n",
        "\n",
        "  def predict(self, X):\n",
        "    # this is actually an individual prediction encased in a for-loop\n",
        "    m_obs = X.shape[1]\n",
        "    y_hat = np.empty(m_obs, dtype=int)\n",
        "\n",
        "    for i in range(m_obs):\n",
        "      y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))\n",
        "\n",
        "    # return prediction as a row vector (matching y)\n",
        "    # print(y_hat)\n",
        "    return y_hat.reshape(1,-1)\n",
        "\n",
        "  def _predict_one(self, x):\n",
        "    # calculate all log posteriori probabilities (actually, +C)\n",
        "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
        "                  in enumerate(self.log_a_priori) ]\n",
        "\n",
        "    # return the class that has maximum a posteriori probability\n",
        "    return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rz2FC7A5NUpN"
      },
      "outputs": [],
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate each covariance matrix\n",
        "    self.inv_covs = [LA.inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "    # Notas JIT\n",
        "    # inv_convs sería la matriz Sigma en la fórmula de log f_j.\n",
        "    # la covarianza se calcula sobre un array 1D o 2D, donde las filas son una variable y las columnas las observaciones\n",
        "    # se tiene una matriz de covarianza por clase j, que son las que se iniciaron en el a priori\n",
        "\n",
        "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
        "    # Rta: el flatten hace que sea un array 1D. Con el ==idx se setean a True los que corresponden a ese idx de log_a_priori\n",
        "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
        "    # Rta: bias=True implica normalizar por N en vez de (N-1), siendo N la cantidad de observaciones dadas\n",
        "    # Según entendí, es debido a que la covarianza de las ocurrencias tiene N en el denominador, y la covarianza de la variable aleatoria tiene (N-1) en el denominador\n",
        "    # Como acá estamos partiendo de datos observados, corresponde usar bias=True y dividir por N\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "    # Q7: que hace axis=1? por que no axis=0?\n",
        "    # axis=1 hace que se hagan las medias sobre las filas. En este caso queremos eso porque es la media de cada label\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "    return 0.5*np.log(LA.det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9lZbID0WNV1Y"
      },
      "outputs": [],
      "source": [
        "class TensorizedQDA(QDA):\n",
        "\n",
        "    def _fit_params(self, X, y):\n",
        "        # ask plain QDA to fit params\n",
        "        super()._fit_params(X,y)\n",
        "\n",
        "        # stack onto new dimension\n",
        "        # JIT: junta las distintas matrices inversas de covarianza y de media en una única matriz\n",
        "        # por default lo hace poniendo una abajo de la otra\n",
        "        # quedaría :\n",
        "        # [\n",
        "        #     inv_convs[0],\n",
        "        #     inv_convs[1],\n",
        "        #     ...\n",
        "        #     inv_convs[n],\n",
        "        # ]\n",
        "        # las inv_covs son de nxn, por lo que se stackean en una 3ra dimensión\n",
        "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
        "        self.tensor_means = np.stack(self.means)\n",
        "        # print(self.tensor_means.shape)\n",
        "\n",
        "    def _predict_log_conditionals(self,x):\n",
        "        # unbiased_x = [x - self.tensor_means[i] for i ...]\n",
        "        # es un array de matrices columna: [[[dato],[dato],...],...]\n",
        "        # print(f\"x[0]={x[0]}\")\n",
        "        # print(f\"tensor_means[0]={self.tensor_means[0]}\")\n",
        "        # print(f\"x.shape={x.shape}\")\n",
        "        # print(f\"tensor_means.shape={self.tensor_means.shape}\")\n",
        "        # print(x.flatten())\n",
        "        unbiased_x = x - self.tensor_means\n",
        "        # print(unbiased_x)\n",
        "        # print(f\"unbiased_x.shape={unbiased_x.shape}\")\n",
        "        # JIT: la traspuesta con args 0,2,1 dice que :\n",
        "        #   el eje 0 pasa al 0\n",
        "        #   el eje 2 pasa a ser el 1\n",
        "        #   el eje 1 pasa a ser el 2\n",
        "        # es decir, se transponen las matrices 2D que componen el tensor, pero no se modifican la dimension \"tensorial\"\n",
        "        # unbiased_x pasa a ser una matriz fila: [[[datos]],[[datos]],...]\n",
        "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
        "        # print(inner_prod.flatten())\n",
        "\n",
        "        # JIT: cómo hace para calcular el determinante de un tensor con 3 dimensiones?\n",
        "        # el determinante de un tensor de dimensiones [m,n,n] es un array de len m, donde se tiene el determinante de la matriz en cada indice m \n",
        "        # -> se tiene un vector con el determinante de cada matriz inversa de covarianza\n",
        "        # JIT: se le hace flatten al inner_prod xq es un 1D array con el valor del producto interno para cada j clase\n",
        "        # print(0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten())\n",
        "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        # return the class that has maximum a posteriori probability\n",
        "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i-WGGi_sQ-pT"
      },
      "outputs": [],
      "source": [
        "class QDA_Chol1(BaseBayesianClassifier):\n",
        "  def _fit_params(self, X, y):\n",
        "    self.L_invs = [\n",
        "        LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
        "        for idx in range(len(self.log_a_priori))\n",
        "    ]\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    L_inv = self.L_invs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    y = L_inv @ unbiased_x\n",
        "\n",
        "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i5DNLtYbQsHi"
      },
      "outputs": [],
      "source": [
        "class QDA_Chol2(BaseBayesianClassifier):\n",
        "  def _fit_params(self, X, y):\n",
        "    self.Ls = [\n",
        "        cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)\n",
        "        for idx in range(len(self.log_a_priori))\n",
        "    ]\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    L = self.Ls[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    y = solve_triangular(L, unbiased_x, lower=True)\n",
        "\n",
        "    return -np.log(L.diagonal().prod()) -0.5 * (y**2).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v0dRvYVQRCgc"
      },
      "outputs": [],
      "source": [
        "class QDA_Chol3(BaseBayesianClassifier):\n",
        "  def _fit_params(self, X, y):\n",
        "    self.L_invs = [\n",
        "        dtrtri(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True), lower=1)[0]\n",
        "        for idx in range(len(self.log_a_priori))\n",
        "    ]\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    L_inv = self.L_invs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "\n",
        "    y = L_inv @ unbiased_x\n",
        "\n",
        "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCtrHQDuN6R4"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Observar que se proveen **4 datasets diferentes**, el código de ejemplo usa uno solo pero eso no significa que ustedes se limiten al mismo. También pueden usar otros datasets de su elección."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rasInBMFNzUH"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris, fetch_openml, load_wine\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_iris_dataset():\n",
        "  data = load_iris()\n",
        "  X_full = data.data\n",
        "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "  return X_full, y_full\n",
        "\n",
        "def get_penguins_dataset():\n",
        "    # get data\n",
        "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
        "\n",
        "    # drop non-numeric columns\n",
        "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
        "\n",
        "    # drop rows with missing values\n",
        "    mask = df.isna().sum(axis=1) == 0\n",
        "    df = df[mask]\n",
        "    tgt = tgt[mask]\n",
        "\n",
        "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
        "\n",
        "def get_wine_dataset():\n",
        "    # get data\n",
        "    data = load_wine()\n",
        "    X_full = data.data\n",
        "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "    return X_full, y_full\n",
        "\n",
        "def get_letters_dataset():\n",
        "    # get data\n",
        "    letter = fetch_openml('letter', version=1, as_frame=False)\n",
        "    return letter.data, letter.target.reshape(-1,1)\n",
        "\n",
        "def label_encode(y_full):\n",
        "    return LabelEncoder().fit_transform(y_full.flatten()).reshape(y_full.shape)\n",
        "\n",
        "def split_transpose(X, y, test_size, random_state):\n",
        "    # X_train, X_test, y_train, y_test but all transposed\n",
        "    return [elem.T for elem in train_test_split(X, y, test_size=test_size, random_state=random_state)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybPkuBdDN42P"
      },
      "source": [
        "## Benchmarking\n",
        "\n",
        "Nota: esta clase fue creada bastante rápido y no pretende ser una plataforma súper confiable sobre la que basarse, sino más bien una herramienta simple con la que poder medir varios runs y agregar la información.\n",
        "\n",
        "En forma rápida, `warmup` es la cantidad de runs para warmup, `mem_runs` es la cantidad de runs en las que se mide el pico de uso de RAM y `n_runs` es la cantidad de runs en las que se miden tiempos.\n",
        "\n",
        "La razón por la que se separan es que medir memoria hace ~2.5x más lento cada run, pero al mismo tiempo se estabiliza mucho más rápido.\n",
        "\n",
        "**Importante:** tener en cuenta que los modelos que predicen en batch (usan `predict` directamente) deberían consumir, como mínimo, $n$ veces la memoria de los que predicen por observación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nO4Py3CeNpKu"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from numpy.random import RandomState\n",
        "import tracemalloc\n",
        "\n",
        "RNG_SEED = 6553\n",
        "\n",
        "class Benchmark:\n",
        "    def __init__(self, X, y, n_runs=1000, warmup=100, mem_runs=100, test_sz=0.3, rng_seed=RNG_SEED, same_splits=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.n = n_runs\n",
        "        self.warmup = warmup\n",
        "        self.mem_runs = mem_runs\n",
        "        self.test_sz = test_sz\n",
        "        self.det = same_splits\n",
        "        if self.det:\n",
        "            self.rng_seed = rng_seed\n",
        "        else:\n",
        "            self.rng = RandomState(rng_seed)\n",
        "\n",
        "        self.data = dict()\n",
        "\n",
        "        print(\"Benching params:\")\n",
        "        print(\"Total runs:\",self.warmup+self.mem_runs+self.n)\n",
        "        print(\"Warmup runs:\",self.warmup)\n",
        "        print(\"Peak Memory usage runs:\", self.mem_runs)\n",
        "        print(\"Running time runs:\", self.n)\n",
        "        approx_test_sz = int(self.y.size * self.test_sz)\n",
        "        print(\"Train size rows (approx):\",self.y.size - approx_test_sz)\n",
        "        print(\"Test size rows (approx):\",approx_test_sz)\n",
        "        print(\"Test size fraction:\",self.test_sz)\n",
        "\n",
        "    def bench(self, model_class, **kwargs):\n",
        "        name = model_class.__name__\n",
        "        time_data = np.empty((self.n, 3), dtype=float)  # train_time, test_time, accuracy\n",
        "        mem_data = np.empty((self.mem_runs, 2), dtype=float)  # train_peak_mem, test_peak_mem\n",
        "        rng = RandomState(self.rng_seed) if self.det else self.rng\n",
        "\n",
        "\n",
        "        for i in range(self.warmup):\n",
        "            # Instantiate model with error check for unsupported parameters\n",
        "            model = model_class(**kwargs)\n",
        "\n",
        "            # Generate current train-test split\n",
        "            X_train, X_test, y_train, y_test = split_transpose(\n",
        "                self.X, self.y,\n",
        "                test_size=self.test_sz,\n",
        "                random_state=rng\n",
        "            )\n",
        "            # Run training and prediction (timing or memory measurement not recorded)\n",
        "            model.fit(X_train, y_train)\n",
        "            model.predict(X_test)\n",
        "\n",
        "        for i in tqdm(range(self.mem_runs), total=self.mem_runs, desc=f\"{name} (MEM)\"):\n",
        "\n",
        "            model = model_class(**kwargs)\n",
        "\n",
        "            X_train, X_test, y_train, y_test = split_transpose(\n",
        "                self.X, self.y,\n",
        "                test_size=self.test_sz,\n",
        "                random_state=rng\n",
        "            )\n",
        "\n",
        "            tracemalloc.start()\n",
        "\n",
        "            t1 = time.perf_counter()\n",
        "            model.fit(X_train, y_train)\n",
        "            t2 = time.perf_counter()\n",
        "\n",
        "            _, train_peak = tracemalloc.get_traced_memory()\n",
        "            tracemalloc.reset_peak()\n",
        "\n",
        "            model.predict(X_test)\n",
        "            t3 = time.perf_counter()\n",
        "            _, test_peak = tracemalloc.get_traced_memory()\n",
        "            tracemalloc.stop()\n",
        "\n",
        "            mem_data[i,] = (\n",
        "                train_peak / (1024 * 1024),\n",
        "                test_peak / (1024 * 1024)\n",
        "            )\n",
        "\n",
        "        for i in tqdm(range(self.n), total=self.n, desc=f\"{name} (TIME)\"):\n",
        "            model = model_class(**kwargs)\n",
        "\n",
        "            X_train, X_test, y_train, y_test = split_transpose(\n",
        "                self.X, self.y,\n",
        "                test_size=self.test_sz,\n",
        "                random_state=rng\n",
        "            )\n",
        "\n",
        "            t1 = time.perf_counter()\n",
        "            model.fit(X_train, y_train)\n",
        "            t2 = time.perf_counter()\n",
        "            preds = model.predict(X_test)\n",
        "            t3 = time.perf_counter()\n",
        "\n",
        "            time_data[i,] = (\n",
        "                (t2 - t1) * 1000,\n",
        "                (t3 - t2) * 1000,\n",
        "                (y_test.flatten() == preds.flatten()).mean()\n",
        "            )\n",
        "\n",
        "        self.data[name] = (time_data, mem_data)\n",
        "\n",
        "    def summary(self, baseline=None):\n",
        "        aux = []\n",
        "        for name, (time_data, mem_data) in self.data.items():\n",
        "            result = {\n",
        "                'model': name,\n",
        "                'train_median_ms': np.median(time_data[:, 0]),\n",
        "                'train_std_ms': time_data[:, 0].std(),\n",
        "                'test_median_ms': np.median(time_data[:, 1]),\n",
        "                'test_std_ms': time_data[:, 1].std(),\n",
        "                'mean_accuracy': time_data[:, 2].mean(),\n",
        "                'train_mem_median_mb': np.median(mem_data[:, 0]),\n",
        "                'train_mem_std_mb': mem_data[:, 0].std(),\n",
        "                'test_mem_median_mb': np.median(mem_data[:, 1]),\n",
        "                'test_mem_std_mb': mem_data[:, 1].std()\n",
        "            }\n",
        "            aux.append(result)\n",
        "        df = pd.DataFrame(aux).set_index('model')\n",
        "\n",
        "        if baseline is not None and baseline in self.data:\n",
        "            df['train_speedup'] = df.loc[baseline, 'train_median_ms'] / df['train_median_ms']\n",
        "            df['test_speedup'] = df.loc[baseline, 'test_median_ms'] / df['test_median_ms']\n",
        "            df['train_mem_reduction'] = df.loc[baseline, 'train_mem_median_mb'] / df['train_mem_median_mb']\n",
        "            df['test_mem_reduction'] = df.loc[baseline, 'test_mem_median_mb'] / df['test_mem_median_mb']\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb5VEpEugFXW"
      },
      "source": [
        "## Ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLyr4-hdgJ7e",
        "outputId": "86428138-3982-4c05-8a88-f6809d524a27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((178, 13), (178, 1))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# levantamos el dataset Wine, que tiene 13 features y 178 observaciones en total\n",
        "X_full, y_full = get_wine_dataset()\n",
        "\n",
        "X_full.shape, y_full.shape\n",
        "#print(y_full.flatten()=='class_0')\n",
        "#print(X_full[:,y_full.flatten()=='class_0'])\n",
        "#y_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxQlFUSbgYHQ",
        "outputId": "32c0fee8-2a79-4f8c-c526-392f026fff1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([['class_0'],\n",
              "        ['class_0'],\n",
              "        ['class_0'],\n",
              "        ['class_0'],\n",
              "        ['class_0']], dtype='<U7'),\n",
              " array([[0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0]]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# encodeamos a número las clases\n",
        "y_full_encoded = label_encode(y_full)\n",
        "\n",
        "y_full[:5], y_full_encoded[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSBNNUOmgtsI",
        "outputId": "7dc60ba1-0548-4671-8f44-a9fb227360d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benching params:\n",
            "Total runs: 140\n",
            "Warmup runs: 20\n",
            "Peak Memory usage runs: 20\n",
            "Running time runs: 100\n",
            "Train size rows (approx): 125\n",
            "Test size rows (approx): 53\n",
            "Test size fraction: 0.3\n"
          ]
        }
      ],
      "source": [
        "# generamos el benchmark\n",
        "# observar que son valores muy bajos de runs para que corra rápido ahora\n",
        "b = Benchmark(\n",
        "    X_full, y_full_encoded,\n",
        "    n_runs = 100,\n",
        "    warmup = 20,\n",
        "    mem_runs = 20,\n",
        "    test_sz = 0.3,\n",
        "    same_splits = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "c01ec4015f1c4c4491687244977cf5ee",
            "f07c3a390d8640e586ec729d3c7c86d8",
            "1d3c4fad476c47208bce6625ec783f6d",
            "a33f440ec8454a2db5bd26485d71c4db",
            "766927c256e3409ab53c04ca092964e0",
            "54a06911978746dd9e81bb1526bd741c",
            "a806c90e849243999c2fce804e20b449",
            "0c4640fad3de46af8ee288fb94e9831d",
            "70c3a115637d4880802a605974661714",
            "3f110d0a165e41b0a268f77b2d2a11b7",
            "de398ad23ecd44d8b8d69e621401f3b3",
            "d0c5a2b6b27144268b7fc0bedcc33a86",
            "684de1d1bee34b309988bc0f40b0d5e2",
            "3a99fe4554914c12ad9146aea349708c",
            "1da7d96834cc4bc8a3cd5595b3446b74",
            "d21ef446c090431480050c3352e2634e",
            "d3fe060bd70848b7af130d99bbff6839",
            "32df4fc69a3c47cf9a220f4469b667a3",
            "254e2937d92d48b0886e5d950a6bbefa",
            "0cead8148ae5469eb637c29de2b21ec7",
            "de9f39ade1b04c22921589c687b25518",
            "437e4360cf5e44bc845c2f6002ea2d55"
          ]
        },
        "id": "zUciOjazhUu5",
        "outputId": "398afdb9-ef70-40db-b771-eb3f883a68f0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12033cae67a7413ea97c55e60b29b8aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "QDA (MEM):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "747036413ced40c28bc7521eba84fb77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "QDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# bencheamos un par\n",
        "to_bench = [QDA]\n",
        "\n",
        "for model in to_bench:\n",
        "    b.bench(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8aeeac18744940f8b4940256613178db",
            "16f5ac0cf3cc441d891ae9522cf0798c",
            "bc84ff51ad3c4921b7aca214f0ff5e35",
            "f8508a02a66547b3a501694a1b8633ab",
            "4ab824715b294c86b045c7e187125524",
            "5e4f83990b0c4572952d5e23005283fb",
            "b85cd793196847cda92178f945624016",
            "b1c06f1940204bdaacf4ad3d843faa2f",
            "74c98c4598bd438cbedbb6c8a492eb15",
            "373a7a1549b44c2696083efddc88fafe",
            "8e2c2ff9e34b49568449aa7eb991d5cb",
            "de1229493a5749c799bcece8c07506f5",
            "c29e6bbacf9c45ecb5465624ee315b70",
            "a2e9076aca0b4224ba3aa8e4c3239260",
            "8df46d45d37e45e889dbec220fa360bd",
            "514491be27294e0cb5b70b6e25ee3355",
            "6d70e647d03f4004bd727673fbbfe5f9",
            "c3149d7e47c84328904326dda8527af4",
            "6197d697782b4688a278663e287317b1",
            "c69d75dc1d1b4fca925f308a79a7e248",
            "7f722aef90b84352b16d45f8716df168",
            "f356ef11b0734ce6a31ec9f0a01e055a"
          ]
        },
        "id": "wpPhSSCNhlvG",
        "outputId": "3edd56df-f71f-41bd-9f02-caa837541aa2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "483f5b1d884545b694a24a4fe61cd3fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "TensorizedQDA (MEM):   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ca40bd8f11f4dffb8ad1d8281d8dd19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "TensorizedQDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# como es una clase, podemos seguir bencheando más después\n",
        "b.bench(TensorizedQDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "bZ5-vowshr5c",
        "outputId": "f494db5c-f2a5-46ba-a718-b7ba68f0699d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_median_ms</th>\n",
              "      <th>train_std_ms</th>\n",
              "      <th>test_median_ms</th>\n",
              "      <th>test_std_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_mem_median_mb</th>\n",
              "      <th>train_mem_std_mb</th>\n",
              "      <th>test_mem_median_mb</th>\n",
              "      <th>test_mem_std_mb</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.154335</td>\n",
              "      <td>0.154908</td>\n",
              "      <td>1.142433</td>\n",
              "      <td>0.304756</td>\n",
              "      <td>0.982407</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.000662</td>\n",
              "      <td>0.008101</td>\n",
              "      <td>0.000412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.169598</td>\n",
              "      <td>0.084541</td>\n",
              "      <td>0.530482</td>\n",
              "      <td>0.153697</td>\n",
              "      <td>0.982593</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.012131</td>\n",
              "      <td>0.000019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
              "model                                                                       \n",
              "QDA                   0.154335      0.154908        1.142433     0.304756   \n",
              "TensorizedQDA         0.169598      0.084541        0.530482     0.153697   \n",
              "\n",
              "               mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
              "model                                                                 \n",
              "QDA                 0.982407               0.0187          0.000662   \n",
              "TensorizedQDA       0.982593               0.0187          0.000657   \n",
              "\n",
              "               test_mem_median_mb  test_mem_std_mb  \n",
              "model                                               \n",
              "QDA                      0.008101         0.000412  \n",
              "TensorizedQDA            0.012131         0.000019  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hacemos un summary\n",
        "b.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "09eKXqlXhwL-",
        "outputId": "a721b3e9-9003-4d2c-9f7e-11a96b0870d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_median_ms</th>\n",
              "      <th>test_median_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.154335</td>\n",
              "      <td>1.142433</td>\n",
              "      <td>0.982407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.169598</td>\n",
              "      <td>0.530482</td>\n",
              "      <td>0.982593</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_median_ms  test_median_ms  mean_accuracy\n",
              "model                                                        \n",
              "QDA                   0.154335        1.142433       0.982407\n",
              "TensorizedQDA         0.169598        0.530482       0.982593"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# son muchos datos! nos quedamos con un par nomás\n",
        "summ = b.summary()\n",
        "\n",
        "# como es un pandas DataFrame, subseteamos columnas fácil\n",
        "summ[['train_median_ms', 'test_median_ms','mean_accuracy']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "EopB9574h8I5",
        "outputId": "b8bf48a0-f791-4a34-da7c-254071cbadf7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_median_ms</th>\n",
              "      <th>train_std_ms</th>\n",
              "      <th>test_median_ms</th>\n",
              "      <th>test_std_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_mem_median_mb</th>\n",
              "      <th>train_mem_std_mb</th>\n",
              "      <th>test_mem_median_mb</th>\n",
              "      <th>test_mem_std_mb</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.154335</td>\n",
              "      <td>0.154908</td>\n",
              "      <td>1.142433</td>\n",
              "      <td>0.304756</td>\n",
              "      <td>0.982407</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.000662</td>\n",
              "      <td>0.008101</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.169598</td>\n",
              "      <td>0.084541</td>\n",
              "      <td>0.530482</td>\n",
              "      <td>0.153697</td>\n",
              "      <td>0.982593</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.012131</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.909999</td>\n",
              "      <td>2.153573</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.667767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
              "model                                                                       \n",
              "QDA                   0.154335      0.154908        1.142433     0.304756   \n",
              "TensorizedQDA         0.169598      0.084541        0.530482     0.153697   \n",
              "\n",
              "               mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
              "model                                                                 \n",
              "QDA                 0.982407               0.0187          0.000662   \n",
              "TensorizedQDA       0.982593               0.0187          0.000657   \n",
              "\n",
              "               test_mem_median_mb  test_mem_std_mb  train_speedup  \\\n",
              "model                                                               \n",
              "QDA                      0.008101         0.000412       1.000000   \n",
              "TensorizedQDA            0.012131         0.000019       0.909999   \n",
              "\n",
              "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
              "model                                                                 \n",
              "QDA                1.000000                  1.0            1.000000  \n",
              "TensorizedQDA      2.153573                  1.0            0.667767  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# podemos setear un baseline para que fabrique columnas de comparación\n",
        "summ = b.summary(baseline='QDA')\n",
        "\n",
        "summ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "z0qeE1gviFLZ",
        "outputId": "760de925-f8fc-4e77-a8ce-50785d14e487"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_median_ms</th>\n",
              "      <th>test_median_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>0.154335</td>\n",
              "      <td>1.142433</td>\n",
              "      <td>0.982407</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>0.169598</td>\n",
              "      <td>0.530482</td>\n",
              "      <td>0.982593</td>\n",
              "      <td>0.909999</td>\n",
              "      <td>2.153573</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.667767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_median_ms  test_median_ms  mean_accuracy  train_speedup  \\\n",
              "model                                                                          \n",
              "QDA                   0.154335        1.142433       0.982407       1.000000   \n",
              "TensorizedQDA         0.169598        0.530482       0.982593       0.909999   \n",
              "\n",
              "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
              "model                                                                 \n",
              "QDA                1.000000                  1.0            1.000000  \n",
              "TensorizedQDA      2.153573                  1.0            0.667767  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summ[[\n",
        "    'train_median_ms', 'test_median_ms','mean_accuracy',\n",
        "    'train_speedup', 'test_speedup',\n",
        "    'train_mem_reduction', 'test_mem_reduction'\n",
        "]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF80Pck2RmaC"
      },
      "source": [
        "# Consigna QDA\n",
        "\n",
        "**Notación**: en general notamos\n",
        "\n",
        "* $k$ la cantidad de clases\n",
        "* $n$ la cantidad de observaciones\n",
        "* $p$ la cantidad de features/variables/predictores\n",
        "\n",
        "**Sugerencia:** combinaciones adecuadas de `transpose`, `stack`, `reshape` y, ocasionalmente, `flatten` y `diagonal` suele ser más que suficiente. Se recomienda *fuertemente* explorar la dimensionalidad de cada elemento antes de implementar las clases.\n",
        "\n",
        "## Tensorización\n",
        "\n",
        "En esta sección nos vamos a ocupar de hacer que el modelo sea más rápido para generar predicciones, observando que incurre en un doble `for` dado que predice en forma individual un escalar para cada observación, para cada clase. Paralelizar ambos vía tensorización suena como una gran vía de mejora de tiempos.\n",
        "\n",
        "### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
        "\n",
        "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
        "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
        "\n",
        "### 2) Optimización\n",
        "\n",
        "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
        "\n",
        "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict.\n",
        "4. Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir.\n",
        "5. Demostrar que\n",
        "$$\n",
        "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
        "$$ es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
        "$$\n",
        "np.sum(A^T \\odot B, axis=0).T\n",
        "$$ \n",
        "queda a preferencia del alumno cuál usar.\n",
        "\n",
        "6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`.\n",
        "7. Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?\n",
        "\n",
        "## Cholesky\n",
        "\n",
        "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
        "\n",
        "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
        "\n",
        "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$.\n",
        "\n",
        "### 3) Diferencias entre implementaciones de `QDA_Chol`\n",
        "\n",
        "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
        "7. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
        "8. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
        "9. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?\n",
        "\n",
        "### 4) Optimización\n",
        "\n",
        "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores.\n",
        "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto.\n",
        "13. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcqVvRSLwaEZ"
      },
      "source": [
        "## Importante:\n",
        "\n",
        "Las métricas que se observan al realizar benchmarking son muy dependientes del código que se ejecuta, y por tanto de las versiones de las librerías utilizadas. Una forma de unificar esto es utilizando un gestor de versiones y paquetes como _uv_ o _Poetry_, otra es simplemente usando una misma VM como la que provee Colab.\n",
        "\n",
        "**Cada equipo debe informar las versiones de Python, NumPy y SciPy con que fueron obtenidos los resultados. En caso de que sean múltiples, agregar todos los casos**. La siguiente celda provee una ayuda para hacerlo desde un notebook, aunque como es una secuencia de comandos también sirve para consola."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwlW7sqwirdn",
        "outputId": "ccccb229-089a-44fb-cda3-da0a11b4fd4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.13\n",
            "numpy==1.26.4\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python --version\n",
        "pip freeze | grep -E \"scipy|numpy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNWRw6ofUNgZ"
      },
      "source": [
        "**Comentario:** yo utilicé los siguientes parámetros para mi run de prueba. Esto NO significa que ustedes tengan que usar los mismos, tampoco el mismo dataset. Se agregó al notebook simplemente porque fue una pregunta común en cohortes anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9Vn-q4RJv8aA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benching params:\n",
            "Total runs: 150\n",
            "Warmup runs: 20\n",
            "Peak Memory usage runs: 30\n",
            "Running time runs: 100\n",
            "Train size rows (approx): 16000\n",
            "Test size rows (approx): 4000\n",
            "Test size fraction: 0.2\n"
          ]
        }
      ],
      "source": [
        "# dataset de letters\n",
        "X_letter, y_letter = get_letters_dataset()\n",
        "\n",
        "# encoding de labels\n",
        "y_letter_encoded = label_encode(y_letter.reshape(-1,1))\n",
        "\n",
        "# instanciacion del benchmark\n",
        "b = Benchmark(\n",
        "    X_letter, y_letter_encoded,\n",
        "    same_splits=False,\n",
        "    # same_splits=True,\n",
        "    n_runs=100,\n",
        "    warmup=20,\n",
        "    mem_runs=30,\n",
        "    # n_runs=0,\n",
        "    # warmup=1,\n",
        "    # mem_runs=0,\n",
        "    test_sz=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Respuestas (borrador)\n",
        "> JIT: Voy armando en borrador acá y después veo como ordeno todo\n",
        "\n",
        "### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
        "\n",
        "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
        "\n",
        "`TensorizedQDA` paraleliza sobre las $k$ clases. Si observamos `_fit_params` observamos que hace un `np.stack` sobre `self.inv_covs`, el cual es una lista de `np.arrays` de 2 dimensiones. El `stack` lo que hace es agregar una dimensión, y poner en esa dimensión las matrices 2D de covarianza y de media."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
        "\n",
        "`tensor_inv_covs` tiene tamaño $(j,p,p)$ donde $j$ es la cantidad de clases y $p$ la cantidad de features. Las medias `tensor_mean` tiene tamaño $(j,p,1)$. Si vamos al método `_predict_log_conditional` tenemos que:\n",
        "```python\n",
        "# QDA:\n",
        "inv_cov = self.inv_covs[class_idx]\n",
        "unbiased_x =  x - self.means[class_idx]\n",
        "return 0.5*np.log(LA.det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x\n",
        "\n",
        "# TensorizedQDA:\n",
        "unbiased_x = x - self.tensor_means\n",
        "inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
        "return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
        "```\n",
        "En `TensorizedQDA` se calcula en primer lugar `unbiased_x` donde se tiene para las $j$ clases $x-\\mu_j$, dando un array de tamaño $(j,p,1)$. En el caso de `QDA`, como se evalúa en particular la clase $k$, se tiene un array $(p,1)$, que se corresponde al $k$ lugar en la dimensión 0 del tensor `unbiased_x`.<br>\n",
        "Luego para el producto interno, la diferencia radica en el detalle de la transpuestam donde los argumentos `0,2,1` indican que la transpuesta se hace para cada array 2D de las dimensiones 1 y 2, es decir, se transpone la matriz de cada clase $k$, y la dimensión 0 de clases se deja intacta. De esta forma el resultado es un array 2D $(j,1)$, donde se tiene el valor del producto interno para cada $j$ clase. Luego esto se le applica `flatten` para llevarlo a un array 1D para realizar la suma por fila.<br>\n",
        "Por último el determinante del tensor `LA.det(self.tensor_inv_cov)` nos entrega el determinante de las $j$ matrices 2D en un array 1D.<br>\n",
        "Finalmente se logra un array 1D donde cada posición es la $k$ clase. Por lo tanto, calcular `QDA._predict_log_conditional(x, k)` es equivalente a `TensorizedQDA._predict_log_conditional(x)[k]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2) Optimización\n",
        "\n",
        "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
        "\n",
        "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FasterQDA(TensorizedQDA):\n",
        "    def _predict_log_conditionals_2(self,x):\n",
        "        print(x.shape)\n",
        "        unbiased_x = x - self.tensor_means\n",
        "        inner_prod = unbiased_x.transpose(0,1,3,2) @ self.tensor_inv_cov @ unbiased_x\n",
        "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.reshape(len(inner_prod),-1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        log_conditionals = self._predict_log_conditionals_2(X.transpose(1,0).reshape(len(X[0]),1,len(X),1)) \n",
        "        predictions = np.argmax(self.log_a_priori + log_conditionals, axis=1)\n",
        "        print(predictions)\n",
        "\n",
        "        return predictions.reshape(1,-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FasterQDA(TensorizedQDA):\n",
        "    def _predict_log_conditionals_2(self,x):\n",
        "        # x tiene dims (n,1,p,1)\n",
        "        # tensor means tiene dims (k,p,1)\n",
        "        # ==> unbiased_x tiene dims (n,k,p,1)\n",
        "        unbiased_x = x.transpose(1,0).reshape(-1, 1, len(x), 1) - self.tensor_means\n",
        "        # convierto unbiased_x para que la dimensión más alta del tensor sea las k clases => (k,p,n)\n",
        "        unbiased_x = unbiased_x.reshape(len(unbiased_x), len(unbiased_x[0]),-1).transpose(1,2,0)\n",
        "\n",
        "        # hago el prod interno\n",
        "        # se obtiene una matriz (n,n), de la cual sólo nos interesa la diagonal\n",
        "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
        "        # print(inner_prod)\n",
        "        inner_prod = np.diagonal(inner_prod, axis1=1, axis2=2)\n",
        "        # print(f\"inner_prod_diag = {inner_prod}\")\n",
        "\n",
        "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.transpose()\n",
        "\n",
        "    def predict(self, X):\n",
        "        log_conditionals = self._predict_log_conditionals_2(X) \n",
        "        predictions = np.argmax(self.log_a_priori + log_conditionals, axis=1)\n",
        "        # print(predictions)\n",
        "\n",
        "        return predictions.reshape(1,-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1376b449b154de29b52b3a415e2274f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "QDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaddf61858054dd1be76e9762763dd04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "QDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ec6ce0cfba94558be629fdbb08a1f78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "TensorizedQDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e814f589c1934bbea3156cddf7554eb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "TensorizedQDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77f1e8dbccfb45b88e69004a750b1a39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FasterQDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "333ed94f5d1c4ccf81d46da90d43957c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FasterQDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "b.bench(QDA)\n",
        "b.bench(TensorizedQDA)\n",
        "b.bench(FasterQDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_median_ms</th>\n",
              "      <th>train_std_ms</th>\n",
              "      <th>test_median_ms</th>\n",
              "      <th>test_std_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_mem_median_mb</th>\n",
              "      <th>train_mem_std_mb</th>\n",
              "      <th>test_mem_median_mb</th>\n",
              "      <th>test_mem_std_mb</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>5.628592</td>\n",
              "      <td>1.175353</td>\n",
              "      <td>729.089632</td>\n",
              "      <td>16.184625</td>\n",
              "      <td>0.885895</td>\n",
              "      <td>0.270454</td>\n",
              "      <td>576.666850</td>\n",
              "      <td>0.098969</td>\n",
              "      <td>575.678698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>5.223853</td>\n",
              "      <td>0.894418</td>\n",
              "      <td>155.767380</td>\n",
              "      <td>3.151754</td>\n",
              "      <td>0.884882</td>\n",
              "      <td>0.270042</td>\n",
              "      <td>0.001821</td>\n",
              "      <td>0.154121</td>\n",
              "      <td>0.000123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FasterQDA</th>\n",
              "      <td>7.850294</td>\n",
              "      <td>1.582009</td>\n",
              "      <td>844.119948</td>\n",
              "      <td>82.704614</td>\n",
              "      <td>0.885590</td>\n",
              "      <td>0.269554</td>\n",
              "      <td>0.002089</td>\n",
              "      <td>3199.335289</td>\n",
              "      <td>0.000620</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
              "model                                                                       \n",
              "QDA                   5.628592      1.175353      729.089632    16.184625   \n",
              "TensorizedQDA         5.223853      0.894418      155.767380     3.151754   \n",
              "FasterQDA             7.850294      1.582009      844.119948    82.704614   \n",
              "\n",
              "               mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
              "model                                                                 \n",
              "QDA                 0.885895             0.270454        576.666850   \n",
              "TensorizedQDA       0.884882             0.270042          0.001821   \n",
              "FasterQDA           0.885590             0.269554          0.002089   \n",
              "\n",
              "               test_mem_median_mb  test_mem_std_mb  \n",
              "model                                               \n",
              "QDA                      0.098969       575.678698  \n",
              "TensorizedQDA            0.154121         0.000123  \n",
              "FasterQDA             3199.335289         0.000620  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir.<br>\n",
        "\n",
        "La matriz $n\\times n$ aparece en el producto interno:\n",
        "```python\n",
        "inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
        "```\n",
        "Escrito \"matemáticamente\", se tiene el producto $${X^*}^T \\cdot \\Sigma^{-1} \\cdot  {X^*},$$ donde $X^*\\in\\real^{k\\times n\\times p}$ y $\\Sigma\\in \\real^{k\\times p\\times p}$, por lo que el resultado $$\\text{inner\\_prod}\\in\\real^{k\\times n\\times n},$$ y sólo nos interesan los elementos de la diagonal (de las $k$ matrices $n\\times n$) que tienen el cuadrado de las observaciones, y todos los productos cruzados son descartados al hacer la diagonal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Demostrar que\n",
        "$$\n",
        "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
        "$$ es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
        "$$\n",
        "np.sum(A^T \\odot B, axis=0).T\n",
        "$$ \n",
        "queda a preferencia del alumno cuál usar.<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientQDA(TensorizedQDA):\n",
        "    def _predict_log_conditionals_2(self,x):\n",
        "        # x tiene dims (n,1,p,1)\n",
        "        # tensor means tiene dims (k,p,1)\n",
        "        # ==> unbiased_x tiene dims (n,k,p,1)\n",
        "        unbiased_x = x.transpose(1,0).reshape(-1, 1, len(x), 1) - self.tensor_means\n",
        "        # convierto unbiased_x para que la dimensión más alta del tensor sea las k clases => (k,p,n)\n",
        "        unbiased_x = unbiased_x.reshape(len(unbiased_x), len(unbiased_x[0]),-1).transpose(1,2,0)\n",
        "\n",
        "        # hago el prod interno\n",
        "        # se obtiene una matriz (n,n), de la cual sólo nos interesa la diagonal\n",
        "        inner_prod = np.sum((unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov) * unbiased_x.transpose(0,2,1), axis=2)\n",
        "        # inner_prod = np.diagonal(inner_prod, axis1=1, axis2=2)\n",
        "\n",
        "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.transpose()\n",
        "\n",
        "    def predict(self, X):\n",
        "        log_conditionals = self._predict_log_conditionals_2(X) \n",
        "        predictions = np.argmax(self.log_a_priori + log_conditionals, axis=1)\n",
        "        # print(predictions)\n",
        "\n",
        "        return predictions.reshape(1,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c4a91d548784ce7bf1ffa2186137b62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "EfficientQDA (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd98678bf8104ba496b4cee6838ebd01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "EfficientQDA (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "b.bench(EfficientQDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?<br>\n",
        "\n",
        "Abajo podemos ver la comparación entre las variantes implementadas. Se observa (tomando `QDA` como base):\n",
        "- `TensorizedQDA` lleva a un speedup de test de x4.68, con casi igual tiempo de entrenamiento. Se tiene casi el mismo costo en memoria para el entrenamiento, mientras que para testeo, se incremento en un 56%.\n",
        "- `FasterQDA` no resultó más rápido, si no que soprendentemente, bastante más lento, tardando un 16% más que `QDA` aproximadamente. Además su consumo de memoria en testeo es muy elevado: aprox. 32,000x respecto de `QDA`. \n",
        "- `EfficientQDA` es mucho más rápido que el resto de variantes, con un 65x de speedup de testeo. Su consumo de memoria fue de aprox. 265x el de `QDA`.\n",
        "\n",
        "Como conclusión, `TensorizedQDA` provee un speedup interesante respecto de `QDA`, con un incremento de consumo de memoria no tan elevado. Por otro lado, `FasterQDA` resulta demasiado costoso tanto en tiempo como en memoria, principalmente por el producto entre matrices de gran tamaño. Por último, `EfficientQDA` sí es eficiente como su nombre lo indica, logrando un speedup muy importante, aunque su consumo de memoria es bastante más elevado que el speedup logrado.\n",
        "\n",
        "Se esperaba un mejor comportamiento de `FasterQDA`, pero viendo el tamaño de los sets de testeo (4,000 observaciones) se entiende el porque de su mala performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_median_ms</th>\n",
              "      <th>train_std_ms</th>\n",
              "      <th>test_median_ms</th>\n",
              "      <th>test_std_ms</th>\n",
              "      <th>mean_accuracy</th>\n",
              "      <th>train_mem_median_mb</th>\n",
              "      <th>train_mem_std_mb</th>\n",
              "      <th>test_mem_median_mb</th>\n",
              "      <th>test_mem_std_mb</th>\n",
              "      <th>train_speedup</th>\n",
              "      <th>test_speedup</th>\n",
              "      <th>train_mem_reduction</th>\n",
              "      <th>test_mem_reduction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>QDA</th>\n",
              "      <td>5.628592</td>\n",
              "      <td>1.175353</td>\n",
              "      <td>729.089632</td>\n",
              "      <td>16.184625</td>\n",
              "      <td>0.885895</td>\n",
              "      <td>0.270454</td>\n",
              "      <td>576.666850</td>\n",
              "      <td>0.098969</td>\n",
              "      <td>575.678698</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorizedQDA</th>\n",
              "      <td>5.223853</td>\n",
              "      <td>0.894418</td>\n",
              "      <td>155.767380</td>\n",
              "      <td>3.151754</td>\n",
              "      <td>0.884882</td>\n",
              "      <td>0.270042</td>\n",
              "      <td>0.001821</td>\n",
              "      <td>0.154121</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>1.077479</td>\n",
              "      <td>4.680631</td>\n",
              "      <td>1.001526</td>\n",
              "      <td>0.642153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FasterQDA</th>\n",
              "      <td>7.850294</td>\n",
              "      <td>1.582009</td>\n",
              "      <td>844.119948</td>\n",
              "      <td>82.704614</td>\n",
              "      <td>0.885590</td>\n",
              "      <td>0.269554</td>\n",
              "      <td>0.002089</td>\n",
              "      <td>3199.335289</td>\n",
              "      <td>0.000620</td>\n",
              "      <td>0.716991</td>\n",
              "      <td>0.863728</td>\n",
              "      <td>1.003340</td>\n",
              "      <td>0.000031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EfficientQDA</th>\n",
              "      <td>4.692137</td>\n",
              "      <td>0.857185</td>\n",
              "      <td>11.212253</td>\n",
              "      <td>1.928078</td>\n",
              "      <td>0.885400</td>\n",
              "      <td>0.268944</td>\n",
              "      <td>0.002196</td>\n",
              "      <td>26.300941</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>1.199580</td>\n",
              "      <td>65.026149</td>\n",
              "      <td>1.005617</td>\n",
              "      <td>0.003763</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
              "model                                                                       \n",
              "QDA                   5.628592      1.175353      729.089632    16.184625   \n",
              "TensorizedQDA         5.223853      0.894418      155.767380     3.151754   \n",
              "FasterQDA             7.850294      1.582009      844.119948    82.704614   \n",
              "EfficientQDA          4.692137      0.857185       11.212253     1.928078   \n",
              "\n",
              "               mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
              "model                                                                 \n",
              "QDA                 0.885895             0.270454        576.666850   \n",
              "TensorizedQDA       0.884882             0.270042          0.001821   \n",
              "FasterQDA           0.885590             0.269554          0.002089   \n",
              "EfficientQDA        0.885400             0.268944          0.002196   \n",
              "\n",
              "               test_mem_median_mb  test_mem_std_mb  train_speedup  \\\n",
              "model                                                               \n",
              "QDA                      0.098969       575.678698       1.000000   \n",
              "TensorizedQDA            0.154121         0.000123       1.077479   \n",
              "FasterQDA             3199.335289         0.000620       0.716991   \n",
              "EfficientQDA            26.300941         0.000026       1.199580   \n",
              "\n",
              "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
              "model                                                                 \n",
              "QDA                1.000000             1.000000            1.000000  \n",
              "TensorizedQDA      4.680631             1.001526            0.642153  \n",
              "FasterQDA          0.863728             1.003340            0.000031  \n",
              "EfficientQDA      65.026149             1.005617            0.003763  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b.summary(baseline='QDA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cholesky\n",
        "\n",
        "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
        "\n",
        "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
        "\n",
        "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$.\n",
        "\n",
        "### 3) Diferencias entre implementaciones de `QDA_Chol`\n",
        "\n",
        "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
        "7. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
        "8. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
        "9. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3584902e269e45d3a7838b8f8d2f9c6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "QDA_Chol1 (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb8ec1bdc55f45648868a5cf3b50e2ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "QDA_Chol1 (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bc19f9a59064a0abcdc64204a70eed0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "QDA_Chol2 (MEM):   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36777e63e79d4878ab046ac201dd6f14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "QDA_Chol2 (TIME):   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m b.bench(QDA_Chol1)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQDA_Chol2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m b.bench(QDA_Chol3)\n\u001b[32m      4\u001b[39m b.summary(baseline=\u001b[33m'\u001b[39m\u001b[33mQDA\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mBenchmark.bench\u001b[39m\u001b[34m(self, model_class, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m model.fit(X_train, y_train)\n\u001b[32m     95\u001b[39m t2 = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m t3 = time.perf_counter()\n\u001b[32m     99\u001b[39m time_data[i,] = (\n\u001b[32m    100\u001b[39m     (t2 - t1) * \u001b[32m1000\u001b[39m,\n\u001b[32m    101\u001b[39m     (t3 - t2) * \u001b[32m1000\u001b[39m,\n\u001b[32m    102\u001b[39m     (y_test.flatten() == preds.flatten()).mean()\n\u001b[32m    103\u001b[39m )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mBaseBayesianClassifier.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     38\u001b[39m y_hat = np.empty(m_obs, dtype=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m_obs):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m   y_hat[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# return prediction as a row vector (matching y)\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# print(y_hat)\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat.reshape(\u001b[32m1\u001b[39m,-\u001b[32m1\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mBaseBayesianClassifier._predict_one\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     48\u001b[39m   \u001b[38;5;66;03m# calculate all log posteriori probabilities (actually, +C)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m   log_posteriori = \u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_a_priori_i\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_log_conditional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_a_priori_i\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_a_priori\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     52\u001b[39m   \u001b[38;5;66;03m# return the class that has maximum a posteriori probability\u001b[39;00m\n\u001b[32m     53\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m np.argmax(log_posteriori)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     48\u001b[39m   \u001b[38;5;66;03m# calculate all log posteriori probabilities (actually, +C)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m   log_posteriori = [ log_a_priori_i + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_log_conditional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx, log_a_priori_i\n\u001b[32m     50\u001b[39m                 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.log_a_priori) ]\n\u001b[32m     52\u001b[39m   \u001b[38;5;66;03m# return the class that has maximum a posteriori probability\u001b[39;00m\n\u001b[32m     53\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m np.argmax(log_posteriori)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mQDA_Chol2._predict_log_conditional\u001b[39m\u001b[34m(self, x, class_idx)\u001b[39m\n\u001b[32m     13\u001b[39m unbiased_x =  x - \u001b[38;5;28mself\u001b[39m.means[class_idx]\n\u001b[32m     15\u001b[39m y = solve_triangular(L, unbiased_x, lower=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -np.log(\u001b[43mL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.prod()) -\u001b[32m0.5\u001b[39m * (y**\u001b[32m2\u001b[39m).sum()\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "b.bench(QDA_Chol1)\n",
        "b.bench(QDA_Chol2)\n",
        "b.bench(QDA_Chol3)\n",
        "b.summary(baseline='QDA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Optimización\n",
        "\n",
        "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores.\n",
        "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto.\n",
        "13. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c4640fad3de46af8ee288fb94e9831d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cead8148ae5469eb637c29de2b21ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16f5ac0cf3cc441d891ae9522cf0798c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e4f83990b0c4572952d5e23005283fb",
            "placeholder": "​",
            "style": "IPY_MODEL_b85cd793196847cda92178f945624016",
            "value": "TensorizedQDA (MEM): 100%"
          }
        },
        "1d3c4fad476c47208bce6625ec783f6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c4640fad3de46af8ee288fb94e9831d",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70c3a115637d4880802a605974661714",
            "value": 20
          }
        },
        "1da7d96834cc4bc8a3cd5595b3446b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de9f39ade1b04c22921589c687b25518",
            "placeholder": "​",
            "style": "IPY_MODEL_437e4360cf5e44bc845c2f6002ea2d55",
            "value": " 100/100 [00:02&lt;00:00, 40.27it/s]"
          }
        },
        "254e2937d92d48b0886e5d950a6bbefa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32df4fc69a3c47cf9a220f4469b667a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "373a7a1549b44c2696083efddc88fafe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a99fe4554914c12ad9146aea349708c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_254e2937d92d48b0886e5d950a6bbefa",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0cead8148ae5469eb637c29de2b21ec7",
            "value": 100
          }
        },
        "3f110d0a165e41b0a268f77b2d2a11b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "437e4360cf5e44bc845c2f6002ea2d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ab824715b294c86b045c7e187125524": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "514491be27294e0cb5b70b6e25ee3355": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54a06911978746dd9e81bb1526bd741c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e4f83990b0c4572952d5e23005283fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6197d697782b4688a278663e287317b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "684de1d1bee34b309988bc0f40b0d5e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3fe060bd70848b7af130d99bbff6839",
            "placeholder": "​",
            "style": "IPY_MODEL_32df4fc69a3c47cf9a220f4469b667a3",
            "value": "QDA (TIME): 100%"
          }
        },
        "6d70e647d03f4004bd727673fbbfe5f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c3a115637d4880802a605974661714": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74c98c4598bd438cbedbb6c8a492eb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "766927c256e3409ab53c04ca092964e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f722aef90b84352b16d45f8716df168": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aeeac18744940f8b4940256613178db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16f5ac0cf3cc441d891ae9522cf0798c",
              "IPY_MODEL_bc84ff51ad3c4921b7aca214f0ff5e35",
              "IPY_MODEL_f8508a02a66547b3a501694a1b8633ab"
            ],
            "layout": "IPY_MODEL_4ab824715b294c86b045c7e187125524"
          }
        },
        "8df46d45d37e45e889dbec220fa360bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f722aef90b84352b16d45f8716df168",
            "placeholder": "​",
            "style": "IPY_MODEL_f356ef11b0734ce6a31ec9f0a01e055a",
            "value": " 100/100 [00:01&lt;00:00, 57.18it/s]"
          }
        },
        "8e2c2ff9e34b49568449aa7eb991d5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2e9076aca0b4224ba3aa8e4c3239260": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6197d697782b4688a278663e287317b1",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c69d75dc1d1b4fca925f308a79a7e248",
            "value": 100
          }
        },
        "a33f440ec8454a2db5bd26485d71c4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f110d0a165e41b0a268f77b2d2a11b7",
            "placeholder": "​",
            "style": "IPY_MODEL_de398ad23ecd44d8b8d69e621401f3b3",
            "value": " 20/20 [00:02&lt;00:00,  7.34it/s]"
          }
        },
        "a806c90e849243999c2fce804e20b449": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1c06f1940204bdaacf4ad3d843faa2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b85cd793196847cda92178f945624016": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc84ff51ad3c4921b7aca214f0ff5e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1c06f1940204bdaacf4ad3d843faa2f",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74c98c4598bd438cbedbb6c8a492eb15",
            "value": 20
          }
        },
        "c01ec4015f1c4c4491687244977cf5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f07c3a390d8640e586ec729d3c7c86d8",
              "IPY_MODEL_1d3c4fad476c47208bce6625ec783f6d",
              "IPY_MODEL_a33f440ec8454a2db5bd26485d71c4db"
            ],
            "layout": "IPY_MODEL_766927c256e3409ab53c04ca092964e0"
          }
        },
        "c29e6bbacf9c45ecb5465624ee315b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d70e647d03f4004bd727673fbbfe5f9",
            "placeholder": "​",
            "style": "IPY_MODEL_c3149d7e47c84328904326dda8527af4",
            "value": "TensorizedQDA (TIME): 100%"
          }
        },
        "c3149d7e47c84328904326dda8527af4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c69d75dc1d1b4fca925f308a79a7e248": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0c5a2b6b27144268b7fc0bedcc33a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_684de1d1bee34b309988bc0f40b0d5e2",
              "IPY_MODEL_3a99fe4554914c12ad9146aea349708c",
              "IPY_MODEL_1da7d96834cc4bc8a3cd5595b3446b74"
            ],
            "layout": "IPY_MODEL_d21ef446c090431480050c3352e2634e"
          }
        },
        "d21ef446c090431480050c3352e2634e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3fe060bd70848b7af130d99bbff6839": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1229493a5749c799bcece8c07506f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c29e6bbacf9c45ecb5465624ee315b70",
              "IPY_MODEL_a2e9076aca0b4224ba3aa8e4c3239260",
              "IPY_MODEL_8df46d45d37e45e889dbec220fa360bd"
            ],
            "layout": "IPY_MODEL_514491be27294e0cb5b70b6e25ee3355"
          }
        },
        "de398ad23ecd44d8b8d69e621401f3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de9f39ade1b04c22921589c687b25518": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f07c3a390d8640e586ec729d3c7c86d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54a06911978746dd9e81bb1526bd741c",
            "placeholder": "​",
            "style": "IPY_MODEL_a806c90e849243999c2fce804e20b449",
            "value": "QDA (MEM): 100%"
          }
        },
        "f356ef11b0734ce6a31ec9f0a01e055a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8508a02a66547b3a501694a1b8633ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373a7a1549b44c2696083efddc88fafe",
            "placeholder": "​",
            "style": "IPY_MODEL_8e2c2ff9e34b49568449aa7eb991d5cb",
            "value": " 20/20 [00:00&lt;00:00, 24.64it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
